{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "999b2417",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " --------The importing has been done!------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from segformer import *\n",
    "from typing import Tuple\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "print(\"\\n\\n --------The importing has been done!------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe95f49c",
   "metadata": {},
   "source": [
    "The input size is \n",
    "data shape--------- torch.Size([1, 1, 224, 224]) torch.Size([1, 1, 224, 224])\n",
    "\n",
    "image: torch.Size([1, 1, 224, 224])\n",
    "\n",
    "label: torch.Size([1, 1, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129e344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test the input of size torch.Size([1, 1, 224, 224])\n",
      "Test the input of size torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.rand(1, 1, 224, 224)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))\n",
    "if inputs.size()[1] == 1:\n",
    "    inputs = inputs.repeat(1,3,1,1)\n",
    "print(\"Test the input of size {}\".format(inputs.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d384188",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8e6e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiT(nn.Module):\n",
    "    def __init__(self, image_size, dims, layers, token_mlp='mix_skip'):\n",
    "        super().__init__()\n",
    "        patch_sizes = [7, 3, 3, 3]\n",
    "        strides = [4, 2, 2, 2]\n",
    "        padding_sizes = [3, 1, 1, 1]\n",
    "        reduction_ratios = [8, 4, 2, 1]\n",
    "        heads = [1, 2, 5, 8]\n",
    "\n",
    "        # patch_embed\n",
    "        # layers = [2, 2, 2, 2] dims = [64, 128, 320, 512]\n",
    "        self.patch_embed1 = OverlapPatchEmbeddings(image_size, patch_sizes[0], strides[0], padding_sizes[0], 3, dims[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbeddings(image_size//4, patch_sizes[1], strides[1],  padding_sizes[1],dims[0], dims[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbeddings(image_size//8, patch_sizes[2], strides[2],  padding_sizes[2],dims[1], dims[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbeddings(image_size//16, patch_sizes[3], strides[3],  padding_sizes[3],dims[2], dims[3])\n",
    "        \n",
    "        # transformer encoder\n",
    "        self.block1 = nn.ModuleList([\n",
    "            TransformerBlock(dims[0], heads[0], reduction_ratios[0],token_mlp)\n",
    "        for _ in range(layers[0])])\n",
    "        self.norm1 = nn.LayerNorm(dims[0])\n",
    "\n",
    "        self.block2 = nn.ModuleList([\n",
    "            TransformerBlock(dims[1], heads[1], reduction_ratios[1],token_mlp)\n",
    "        for _ in range(layers[1])])\n",
    "        self.norm2 = nn.LayerNorm(dims[1])\n",
    "\n",
    "        self.block3 = nn.ModuleList([\n",
    "            TransformerBlock(dims[2], heads[2], reduction_ratios[2], token_mlp)\n",
    "        for _ in range(layers[2])])\n",
    "        self.norm3 = nn.LayerNorm(dims[2])\n",
    "\n",
    "        self.block4 = nn.ModuleList([\n",
    "            TransformerBlock(dims[3], heads[3], reduction_ratios[3], token_mlp)\n",
    "        for _ in range(layers[3])])\n",
    "        self.norm4 = nn.LayerNorm(dims[3])\n",
    "\n",
    "        # self.head = nn.Linear(dims[3], num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.shape[0]\n",
    "        outs = []\n",
    "\n",
    "        # stage 1\n",
    "        x, H, W = self.patch_embed1(x)\n",
    "        for blk in self.block1:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 2\n",
    "        x, H, W = self.patch_embed2(x)\n",
    "        for blk in self.block2:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm2(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 3\n",
    "        x, H, W = self.patch_embed3(x)\n",
    "        for blk in self.block3:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm3(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        # stage 4\n",
    "        x, H, W = self.patch_embed4(x)\n",
    "        for blk in self.block4:\n",
    "            x = blk(x, H, W)\n",
    "        x = self.norm4(x)\n",
    "        x = x.reshape(B, H, W, -1).permute(0, 3, 1, 2).contiguous()\n",
    "        outs.append(x)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04a7e8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of stages from encoder: 4\n",
      "The size of output from the 0 stage: torch.Size([1, 64, 56, 56])\n",
      "The size of output from the 1 stage: torch.Size([1, 128, 28, 28])\n",
      "The size of output from the 2 stage: torch.Size([1, 320, 14, 14])\n",
      "The size of output from the 3 stage: torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "dims, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "token_mlp_mode=\"mix_skip\"\n",
    "encoder = MiT(224, dims, layers,token_mlp_mode)\n",
    "output_enc = encoder(inputs)\n",
    "print(\"The number of stages from encoder: {}\".format(len(output_enc)))\n",
    "for i in range(len(output_enc)):\n",
    "    print(\"The size of output from the {} stage: {}\".format(i, output_enc[i].shape))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a7571",
   "metadata": {},
   "source": [
    "# Bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f62ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridgeLayer_4(nn.Module):\n",
    "    def __init__(self, dims, head, reduction_ratios):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(dims)\n",
    "        self.attn = M_EfficientSelfAtten(dims, head, reduction_ratios)\n",
    "        self.norm2 = nn.LayerNorm(dims)\n",
    "        self.mixffn1 = MixFFN_skip(dims,dims*4)\n",
    "        self.mixffn2 = MixFFN_skip(dims*2,dims*8)\n",
    "        self.mixffn3 = MixFFN_skip(dims*5,dims*20)\n",
    "        self.mixffn4 = MixFFN_skip(dims*8,dims*32)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        B = inputs[0].shape[0]\n",
    "        C = 64\n",
    "        if (type(inputs) == list):\n",
    "            # If input type is list, then the block is the first bridge layer\n",
    "            # The feature from the four stages should be concatenated together\n",
    "            print(\"\\n\\n-----1-----\")\n",
    "            c1, c2, c3, c4 = inputs\n",
    "            B, C, _, _= c1.shape\n",
    "            c1f = c1.permute(0, 2, 3, 1).reshape(B, -1, C)  # 3136*64\n",
    "            c2f = c2.permute(0, 2, 3, 1).reshape(B, -1, C)  # 1568*64\n",
    "            c3f = c3.permute(0, 2, 3, 1).reshape(B, -1, C)  # 980*64\n",
    "            c4f = c4.permute(0, 2, 3, 1).reshape(B, -1, C)  # 392*64\n",
    "            \n",
    "            print(c1f.shape, c2f.shape, c3f.shape, c4f.shape)\n",
    "            inputs = torch.cat([c1f, c2f, c3f, c4f], -2)\n",
    "            print(\"The shape of input is {}\".format(inputs.shape))\n",
    "        else:\n",
    "            print(\"\\n\\n-----not 1-----\")\n",
    "            # In this case, the block is not the first bridge layer\n",
    "            B,_,C = inputs.shape \n",
    "            print(inputs.shape)\n",
    "            print(\"The shape of input is {}\".format(inputs.shape))\n",
    "\n",
    "        tx1 = inputs + self.attn(self.norm1(inputs))\n",
    "        tx = self.norm2(tx1)\n",
    "        \n",
    "        print(\"\\n--------seq2imgs--------------\\n\")\n",
    "        tem1 = tx[:,:3136,:].reshape(B, -1, C) \n",
    "        print(\"The shape of tem1 is {}\".format(tem1.shape))\n",
    "        tem2 = tx[:,3136:4704,:].reshape(B, -1, C*2)\n",
    "        print(\"The shape of tem2 is {}\".format(tem2.shape))\n",
    "        tem3 = tx[:,4704:5684,:].reshape(B, -1, C*5)\n",
    "        print(\"The shape of tem3 is {}\".format(tem3.shape))\n",
    "        tem4 = tx[:,5684:6076,:].reshape(B, -1, C*8)\n",
    "        print(\"The shape of tem4 is {}\".format(tem4.shape))\n",
    "\n",
    "        print(\"\\n--------imgs passing through Emix_FFN and img2seq--------------\\n\")\n",
    "        m1f = self.mixffn1(tem1, 56, 56).reshape(B, -1, C)\n",
    "        print(\"The shape of m1f is {}\".format(m1f.shape))\n",
    "        m2f = self.mixffn2(tem2, 28, 28).reshape(B, -1, C)\n",
    "        print(\"The shape of m2f is {}\".format(m2f.shape))\n",
    "        m3f = self.mixffn3(tem3, 14, 14).reshape(B, -1, C)\n",
    "        print(\"The shape of m3f is {}\".format(m3f.shape))\n",
    "        m4f = self.mixffn4(tem4, 7, 7).reshape(B, -1, C)\n",
    "        print(\"The shape of m4f is {}\".format(m4f.shape))\n",
    "\n",
    "        print(\"\\n-------concatenate the seqs---------\\n\")\n",
    "        t1 = torch.cat([m1f, m2f, m3f, m4f], -2)\n",
    "        print(\"The shape of t1 is {}\".format(t1.shape))\n",
    "        tx2 = tx1 + t1\n",
    "        print(\"The shape of tx2 = x1 + t1 is {}\".format(t1.shape))\n",
    "\n",
    "\n",
    "        return tx2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dfbde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BridegeBlock_4(nn.Module):\n",
    "    def __init__(self, dims, head, reduction_ratios):\n",
    "        super().__init__()\n",
    "        self.bridge_layer1 = BridgeLayer_4(dims, head, reduction_ratios)\n",
    "        self.bridge_layer2 = BridgeLayer_4(dims, head, reduction_ratios)\n",
    "        self.bridge_layer3 = BridgeLayer_4(dims, head, reduction_ratios)\n",
    "        self.bridge_layer4 = BridgeLayer_4(dims, head, reduction_ratios)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        bridge1 = self.bridge_layer1(x)\n",
    "        bridge2 = self.bridge_layer2(bridge1)\n",
    "        bridge3 = self.bridge_layer3(bridge2)\n",
    "        bridge4 = self.bridge_layer4(bridge3)\n",
    "\n",
    "        B,_,C = bridge4.shape\n",
    "        outs = []\n",
    "\n",
    "        sk1 = bridge4[:,:3136,:].reshape(B, 56, 56, C).permute(0,3,1,2) \n",
    "        sk2 = bridge4[:,3136:4704,:].reshape(B, 28, 28, C*2).permute(0,3,1,2) \n",
    "        sk3 = bridge4[:,4704:5684,:].reshape(B, 14, 14, C*5).permute(0,3,1,2) \n",
    "        sk4 = bridge4[:,5684:6076,:].reshape(B, 7, 7, C*8).permute(0,3,1,2) \n",
    "        print(\"\\n\\nThe shape of sk:\\n{} {} {} {}\".format(sk1.shape, sk2.shape, sk3.shape, sk4.shape))\n",
    "\n",
    "        outs.append(sk1)\n",
    "        outs.append(sk2)\n",
    "        outs.append(sk3)\n",
    "        outs.append(sk4)\n",
    "\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73b14d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "-----1-----\n",
      "torch.Size([1, 3136, 64]) torch.Size([1, 1568, 64]) torch.Size([1, 980, 64]) torch.Size([1, 392, 64])\n",
      "The shape of input is torch.Size([1, 6076, 64])\n",
      "\n",
      "--------seq2imgs--------------\n",
      "\n",
      "The shape of tem1 is torch.Size([1, 3136, 64])\n",
      "The shape of tem2 is torch.Size([1, 784, 128])\n",
      "The shape of tem3 is torch.Size([1, 196, 320])\n",
      "The shape of tem4 is torch.Size([1, 49, 512])\n",
      "\n",
      "--------imgs passing through Emix_FFN and img2seq--------------\n",
      "\n",
      "The shape of m1f is torch.Size([1, 3136, 64])\n",
      "The shape of m2f is torch.Size([1, 1568, 64])\n",
      "The shape of m3f is torch.Size([1, 980, 64])\n",
      "The shape of m4f is torch.Size([1, 392, 64])\n",
      "\n",
      "-------concatenate the seqs---------\n",
      "\n",
      "The shape of t1 is torch.Size([1, 6076, 64])\n",
      "The shape of tx2 = x1 + t1 is torch.Size([1, 6076, 64])\n",
      "\n",
      "\n",
      "-----not 1-----\n",
      "torch.Size([1, 6076, 64])\n",
      "The shape of input is torch.Size([1, 6076, 64])\n",
      "\n",
      "--------seq2imgs--------------\n",
      "\n",
      "The shape of tem1 is torch.Size([1, 3136, 64])\n",
      "The shape of tem2 is torch.Size([1, 784, 128])\n",
      "The shape of tem3 is torch.Size([1, 196, 320])\n",
      "The shape of tem4 is torch.Size([1, 49, 512])\n",
      "\n",
      "--------imgs passing through Emix_FFN and img2seq--------------\n",
      "\n",
      "The shape of m1f is torch.Size([1, 3136, 64])\n",
      "The shape of m2f is torch.Size([1, 1568, 64])\n",
      "The shape of m3f is torch.Size([1, 980, 64])\n",
      "The shape of m4f is torch.Size([1, 392, 64])\n",
      "\n",
      "-------concatenate the seqs---------\n",
      "\n",
      "The shape of t1 is torch.Size([1, 6076, 64])\n",
      "The shape of tx2 = x1 + t1 is torch.Size([1, 6076, 64])\n",
      "\n",
      "\n",
      "-----not 1-----\n",
      "torch.Size([1, 6076, 64])\n",
      "The shape of input is torch.Size([1, 6076, 64])\n",
      "\n",
      "--------seq2imgs--------------\n",
      "\n",
      "The shape of tem1 is torch.Size([1, 3136, 64])\n",
      "The shape of tem2 is torch.Size([1, 784, 128])\n",
      "The shape of tem3 is torch.Size([1, 196, 320])\n",
      "The shape of tem4 is torch.Size([1, 49, 512])\n",
      "\n",
      "--------imgs passing through Emix_FFN and img2seq--------------\n",
      "\n",
      "The shape of m1f is torch.Size([1, 3136, 64])\n",
      "The shape of m2f is torch.Size([1, 1568, 64])\n",
      "The shape of m3f is torch.Size([1, 980, 64])\n",
      "The shape of m4f is torch.Size([1, 392, 64])\n",
      "\n",
      "-------concatenate the seqs---------\n",
      "\n",
      "The shape of t1 is torch.Size([1, 6076, 64])\n",
      "The shape of tx2 = x1 + t1 is torch.Size([1, 6076, 64])\n",
      "\n",
      "\n",
      "-----not 1-----\n",
      "torch.Size([1, 6076, 64])\n",
      "The shape of input is torch.Size([1, 6076, 64])\n",
      "\n",
      "--------seq2imgs--------------\n",
      "\n",
      "The shape of tem1 is torch.Size([1, 3136, 64])\n",
      "The shape of tem2 is torch.Size([1, 784, 128])\n",
      "The shape of tem3 is torch.Size([1, 196, 320])\n",
      "The shape of tem4 is torch.Size([1, 49, 512])\n",
      "\n",
      "--------imgs passing through Emix_FFN and img2seq--------------\n",
      "\n",
      "The shape of m1f is torch.Size([1, 3136, 64])\n",
      "The shape of m2f is torch.Size([1, 1568, 64])\n",
      "The shape of m3f is torch.Size([1, 980, 64])\n",
      "The shape of m4f is torch.Size([1, 392, 64])\n",
      "\n",
      "-------concatenate the seqs---------\n",
      "\n",
      "The shape of t1 is torch.Size([1, 6076, 64])\n",
      "The shape of tx2 = x1 + t1 is torch.Size([1, 6076, 64])\n",
      "\n",
      "\n",
      "The shape of sk:\n",
      "torch.Size([1, 64, 56, 56]) torch.Size([1, 128, 28, 28]) torch.Size([1, 320, 14, 14]) torch.Size([1, 512, 7, 7])\n",
      "The number of stages from bridge: 4\n",
      "The size of output from the 0 stage: torch.Size([1, 64, 56, 56])\n",
      "The size of output from the 1 stage: torch.Size([1, 128, 28, 28])\n",
      "The size of output from the 2 stage: torch.Size([1, 320, 14, 14])\n",
      "The size of output from the 3 stage: torch.Size([1, 512, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "reduction_ratios = [8, 4, 2, 1]\n",
    "bridge = BridegeBlock_4(64, 1, reduction_ratios)\n",
    "output_br = bridge(output_enc) \n",
    "print(\"The number of stages from bridge: {}\".format(len(output_br)))\n",
    "for i in range(len(output_br)):\n",
    "    print(\"The size of output from the {} stage: {}\".format(i, output_br[i].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662f14a1",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a5f0b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpand(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=2, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.expand = nn.Linear(dim, 2*dim, bias=False) if dim_scale==2 else nn.Identity()\n",
    "        self.norm = norm_layer(dim // dim_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        # print(\"x_shape-----\",x.shape)\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        \n",
    "        B, L, C = x.shape\n",
    "        # print(x.shape)\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//4)\n",
    "        x = x.view(B,-1,C//4)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0e81e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalPatchExpand_X4(nn.Module):\n",
    "    def __init__(self, input_resolution, dim, dim_scale=4, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.dim_scale = dim_scale\n",
    "        self.expand = nn.Linear(dim, 16*dim, bias=False)\n",
    "        self.output_dim = dim \n",
    "        self.norm = norm_layer(self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        x = self.expand(x)\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        x = x.view(B, H, W, C)\n",
    "        x = rearrange(x, 'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=self.dim_scale, p2=self.dim_scale, c=C//(self.dim_scale**2))\n",
    "        x = x.view(B,-1,self.output_dim)\n",
    "        x= self.norm(x.clone())\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49b5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_size, in_out_chan, heads, reduction_ratios,token_mlp_mode, n_class=9, norm_layer=nn.LayerNorm, is_last=False):\n",
    "        super().__init__()\n",
    "        dims = in_out_chan[0]\n",
    "        out_dim = in_out_chan[1]\n",
    "        if not is_last:\n",
    "            self.concat_linear = nn.Linear(dims*2, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = PatchExpand(input_resolution=input_size, dim=out_dim, dim_scale=2, norm_layer=norm_layer)\n",
    "            self.last_layer = None\n",
    "        else:\n",
    "            self.concat_linear = nn.Linear(dims*4, out_dim)\n",
    "            # transformer decoder\n",
    "            self.layer_up = FinalPatchExpand_X4(input_resolution=input_size, dim=out_dim, dim_scale=4, norm_layer=norm_layer)\n",
    "            # self.last_layer = nn.Linear(out_dim, n_class)\n",
    "            self.last_layer = nn.Conv2d(out_dim, n_class,1)\n",
    "            # self.last_layer = None\n",
    "\n",
    "        self.layer_former_1 = TransformerBlock(out_dim, heads, reduction_ratios, token_mlp_mode)\n",
    "        self.layer_former_2 = TransformerBlock(out_dim, heads, reduction_ratios, token_mlp_mode)\n",
    "       \n",
    "\n",
    "        def init_weights(self): \n",
    "            for m in self.modules():\n",
    "                if isinstance(m, nn.Linear):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.LayerNorm):\n",
    "                    nn.init.ones_(m.weight)\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                elif isinstance(m, nn.Conv2d):\n",
    "                    nn.init.xavier_uniform_(m.weight)\n",
    "                    if m.bias is not None:\n",
    "                        nn.init.zeros_(m.bias)\n",
    "\n",
    "        init_weights(self)\n",
    "      \n",
    "    def forward(self, x1, x2=None):\n",
    "        if x2 is not None:# skip connection exist\n",
    "            print(\"x1 shape:\", x1.shape)\n",
    "            print(\"x2 shape:\", x2.shape)\n",
    "            b, h, w, c = x2.shape\n",
    "            x2 = x2.view(b, -1, c)\n",
    "            print(\"------\",x1.shape, x2.shape)\n",
    "            cat_x = torch.cat([x1, x2], dim=-1)\n",
    "            print(\"-----catx shape\", cat_x.shape)\n",
    "            cat_linear_x = self.concat_linear(cat_x)\n",
    "            tran_layer_1 = self.layer_former_1(cat_linear_x, h, w)\n",
    "            tran_layer_2 = self.layer_former_2(tran_layer_1, h, w)\n",
    "            \n",
    "            if self.last_layer:\n",
    "                out = self.last_layer(self.layer_up(tran_layer_2).view(b, 4*h, 4*w, -1).permute(0,3,1,2)) \n",
    "            else:\n",
    "                out = self.layer_up(tran_layer_2)\n",
    "        else:\n",
    "            # if len(x1.shape)>3:\n",
    "            #     x1 = x1.permute(0,2,3,1)\n",
    "            #     b, h, w, c = x1.shape\n",
    "            #     x1 = x1.view(b, -1, c)\n",
    "            print(\"What is this else condition?\")\n",
    "            print(\"x1 shape\",x1.shape)\n",
    "            out = self.layer_up(x1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dda7c0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shapes of output_br: \n",
      " output_br[3]torch.Size([1, 512, 7, 7]) \n",
      "output_br[2]torch.Size([1, 320, 14, 14]) \n",
      "output_br[1]torch.Size([1, 128, 28, 28]) \n",
      "output_br[0]torch.Size([1, 64, 56, 56])\n",
      "\n",
      "stage3-----\n",
      "What is this else condition?\n",
      "x1 shape torch.Size([1, 49, 512])\n",
      "stage2-----\n",
      "x1 shape: torch.Size([1, 196, 256])\n",
      "x2 shape: torch.Size([1, 14, 14, 320])\n",
      "------ torch.Size([1, 196, 256]) torch.Size([1, 196, 320])\n",
      "-----catx shape torch.Size([1, 196, 576])\n",
      "stage1-----\n",
      "x1 shape: torch.Size([1, 784, 160])\n",
      "x2 shape: torch.Size([1, 28, 28, 128])\n",
      "------ torch.Size([1, 784, 160]) torch.Size([1, 784, 128])\n",
      "-----catx shape torch.Size([1, 784, 288])\n",
      "stage0-----\n",
      "x1 shape: torch.Size([1, 3136, 64])\n",
      "x2 shape: torch.Size([1, 56, 56, 64])\n",
      "------ torch.Size([1, 3136, 64]) torch.Size([1, 3136, 64])\n",
      "-----catx shape torch.Size([1, 3136, 128])\n",
      "The shapes of tmp: \n",
      "tmp_3:torch.Size([1, 196, 256]) \n",
      "tmp_2:torch.Size([1, 784, 160]) \n",
      "tmp_1:torch.Size([1, 3136, 64]) \n",
      "tmp_0:torch.Size([1, 9, 224, 224])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b,c,_,_ = output_br[3].shape\n",
    "print(\"The shapes of output_br: \\n output_br[3]{} \\noutput_br[2]{} \\noutput_br[1]{} \\noutput_br[0]{}\\n\".format(output_br[3].shape, output_br[2].shape,output_br[1].shape, output_br[0].shape))\n",
    "\n",
    "reduction_ratios = [8, 4, 2, 1]\n",
    "heads = [1, 2, 5, 8]\n",
    "d_base_feat_size = 7 #16 for 512 inputsize   7for 224\n",
    "in_out_chan = [[32, 64],[144, 128],[288, 320],[512, 512]]\n",
    "\n",
    "dims, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "num_classes=9\n",
    "token_mlp_mode=\"mix_skip\"\n",
    "\n",
    "reduction_ratios = [1, 2, 4, 8]\n",
    "\n",
    "\n",
    "decoder_3= MyDecoderLayer((d_base_feat_size,d_base_feat_size), in_out_chan[3], heads[3], reduction_ratios[3],token_mlp_mode, n_class=num_classes)\n",
    "decoder_2= MyDecoderLayer((d_base_feat_size*2,d_base_feat_size*2),in_out_chan[2], heads[2], reduction_ratios[2], token_mlp_mode, n_class=num_classes)\n",
    "decoder_1= MyDecoderLayer((d_base_feat_size*4,d_base_feat_size*4), in_out_chan[1], heads[1], reduction_ratios[1], token_mlp_mode, n_class=num_classes)\n",
    "decoder_0= MyDecoderLayer((d_base_feat_size*8,d_base_feat_size*8), in_out_chan[0], heads[0], reduction_ratios[0], token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "#---------------Decoder-------------------------     \n",
    "print(\"stage3-----\")   \n",
    "tmp_3 = decoder_3(output_br[3].permute(0,2,3,1).view(b,-1,c))\n",
    "print(\"stage2-----\")   \n",
    "tmp_2 = decoder_2(tmp_3, output_br[2].permute(0,2,3,1))\n",
    "print(\"stage1-----\")   \n",
    "tmp_1 = decoder_1(tmp_2, output_br[1].permute(0,2,3,1))\n",
    "print(\"stage0-----\")  \n",
    "tmp_0 = decoder_0(tmp_1, output_br[0].permute(0,2,3,1))\n",
    "\n",
    "print(\"The shapes of tmp: \\ntmp_3:{} \\ntmp_2:{} \\ntmp_1:{} \\ntmp_0:{}\\n\".format(tmp_3.shape, tmp_2.shape,tmp_1.shape, tmp_0.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c08c5",
   "metadata": {},
   "source": [
    "# The MISSFormer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29534c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MISSFormer(nn.Module):\n",
    "    def __init__(self, num_classes=9, token_mlp_mode=\"mix_skip\", encoder_pretrained=True):\n",
    "        super().__init__()\n",
    "    \n",
    "        reduction_ratios = [8, 4, 2, 1]\n",
    "        heads = [1, 2, 5, 8]\n",
    "        d_base_feat_size = 7 #16 for 512 inputsize   7for 224\n",
    "        in_out_chan = [[32, 64],[144, 128],[288, 320],[512, 512]]\n",
    "\n",
    "        dims, layers = [[64, 128, 320, 512], [2, 2, 2, 2]]\n",
    "        self.backbone = MiT(224, dims, layers,token_mlp_mode)\n",
    "\n",
    "        self.reduction_ratios = [1, 2, 4, 8]\n",
    "        self.bridge = BridegeBlock_4(64, 1, self.reduction_ratios)\n",
    "\n",
    "        self.decoder_3= MyDecoderLayer((d_base_feat_size,d_base_feat_size), in_out_chan[3], heads[3], reduction_ratios[3],token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_2= MyDecoderLayer((d_base_feat_size*2,d_base_feat_size*2),in_out_chan[2], heads[2], reduction_ratios[2], token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_1= MyDecoderLayer((d_base_feat_size*4,d_base_feat_size*4), in_out_chan[1], heads[1], reduction_ratios[1], token_mlp_mode, n_class=num_classes)\n",
    "        self.decoder_0= MyDecoderLayer((d_base_feat_size*8,d_base_feat_size*8), in_out_chan[0], heads[0], reduction_ratios[0], token_mlp_mode, n_class=num_classes, is_last=True)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #---------------Encoder-------------------------\n",
    "        if x.size()[1] == 1:\n",
    "            x = x.repeat(1,3,1,1)\n",
    "\n",
    "        encoder = self.backbone(x)\n",
    "        bridge = self.bridge(encoder) #list\n",
    "\n",
    "        b,c,_,_ = bridge[3].shape\n",
    "        # print(bridge[3].shape, bridge[2].shape,bridge[1].shape, bridge[0].shape)\n",
    "        #---------------Decoder-------------------------     \n",
    "        # print(\"stage3-----\")   \n",
    "        tmp_3 = self.decoder_3(bridge[3].permute(0,2,3,1).view(b,-1,c))\n",
    "        # print(\"stage2-----\")   \n",
    "        tmp_2 = self.decoder_2(tmp_3, bridge[2].permute(0,2,3,1))\n",
    "        # print(\"stage1-----\")   \n",
    "        tmp_1 = self.decoder_1(tmp_2, bridge[1].permute(0,2,3,1))\n",
    "        # print(\"stage0-----\")  \n",
    "        tmp_0 = self.decoder_0(tmp_1, bridge[0].permute(0,2,3,1))\n",
    "\n",
    "        return tmp_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
